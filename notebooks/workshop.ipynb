{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßä Iceberg workshop\n",
    "Before we dive into the data, make sure you're using the same virtual environment as you did during setup. If you're using `uv`, that‚Äôs probably `.venv`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Peek into the Iceberg catalog\n",
    "\n",
    "Alright, let's take a little detour behind the scenes. We‚Äôre going to open the catalog database and see what Iceberg knows about our tables.\n",
    "\n",
    "Here‚Äôs how you can list all the tables in the catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "\n",
    "with sqlite3.connect('../dlt_portable_data_lake_demo/_data/dev/local/catalog.db') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we‚Äôre in the right place! Now let‚Äôs dig a little deeper.\n",
    "You'll get a neat table showing metadata locations for each Iceberg table. It‚Äôs like peeking into the map that shows us where the treasure is buried üó∫Ô∏è‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "with sqlite3.connect('../dlt_portable_data_lake_demo/_data/dev/local/catalog.db') as conn:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM iceberg_tables\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Mini Exercise: What‚Äôs the schema of the `stores` table?\n",
    "\n",
    "Every Iceberg table comes with metadata that describes its schema. Your mission: figure out what the `stores` table looks like under the hood. Here's a little skeleton to help you get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "file_uri = # put your code here\n",
    "path_to_metadata = urllib.parse.urlparse(file_uri).path\n",
    "\n",
    "with open(path_to_metadata, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "# print out the schema of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Stuck? Don‚Äôt worry, you can peek at the answer below:\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "    \n",
    "```python\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "file_uri = df.loc[2, \"metadata_location\"]\n",
    "path_to_metadata = urllib.parse.urlparse(file_uri).path\n",
    "with open(path_to_metadata, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "data[\"schemas\"]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîì Accessing the Data ‚Äì The Fun Begins üéâ\n",
    "\n",
    "We‚Äôve explored the catalog, now let‚Äôs actually use the data! Thanks to uv, your `dlt.yml` project is packaged like a proper Python module, so importing it is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt_portable_data_lake_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let‚Äôs list out the available datasets and tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_catalog = dlt_portable_data_lake_demo.catalog(profile=\"dev\")\n",
    "print(\"DATASETS\")\n",
    "print(dataset_catalog)\n",
    "\n",
    "print(\"TABLES\")\n",
    "print(dataset_catalog.jaffle_shop_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grab a specific table as a DataFrame, just do this:\n",
    "\n",
    "```python\n",
    "df =  dataset_catalog.jaffle_shop_dataset.<name_of_the_table>.df()\n",
    "```\n",
    "\n",
    "### üéØ Mini Exercise: What kinds of products are sold in the Jaffle Shop?\n",
    "\n",
    "Get the types of products straight from the products table!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "<summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "products = dataset_catalog.jaffle_shop_dataset.products.df()\n",
    "print(products[\"type\"].unique())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = dataset_catalog.jaffle_shop_dataset.orders.df()\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access data with Ibis üßô‚Äç‚ôÇÔ∏è \n",
    "\n",
    "Say hello to [ibis](https://ibis-project.org/) ‚Äì the lazy, expressive data wizard! ü™Ñ You can use Ibis to write pandas-like queries, and they‚Äôll only run when you ask for the data. This library is open-source and can be used with most of the databases. Here‚Äôs how you use it with our catalog:\n",
    "\n",
    "\n",
    "```python\n",
    "ibis_statement = dataset_catalog.jaffle_shop_dataset.<name_of_the_table>.<any_ibis_method>()\n",
    "result = ibis_statement.df()\n",
    "```\n",
    "\n",
    "In this case, Ibis will be used to generate SQL statement executed via duckdb on a view of Iceberg table ü§Ø\n",
    "\n",
    "### üéØ Mini Exercise: How many perishable vs non-perishable supplies do we have?\n",
    "\n",
    "Hint: You‚Äôll want to group by the perishable column and count the number of items in each group. Use [ibis documentation](https://ibis-project.org/tutorials/coming-from/pandas#group-by)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplies = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "<summary>Answer</summary>\n",
    "\n",
    "```python\n",
    "supplies = dataset_catalog.jaffle_shop_dataset.supplies\n",
    "supplies.aggregate(by=\"perishable\", count=supplies.id.count()).df()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Let‚Äôs look at just the perishable ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "supplies = dataset_catalog.jaffle_shop_dataset.supplies\n",
    "perishable = supplies.filter(supplies.perishable)\n",
    "perishable.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Iceberg tables with SQL\n",
    "\n",
    "If you‚Äôre more of a SQL fan, don‚Äôt worry ‚Äì we got you! With the `sql_client` provided by dlt, you can run SQL directly on your Iceberg views:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "ds = dataset_catalog.jaffle_shop_dataset\n",
    "with ds.sql_client as c:\n",
    "    c.create_views_for_all_tables()\n",
    "    conn: duckdb.DuckDBPyConnection = c.native_connection\n",
    "    print(conn.sql(\"SHOW TABLES;\"))\n",
    "    print(conn.sql(\"SELECT * FROM stores\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL + Iceberg + DuckDB = a triple threat combo üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßä Going full Iceberg with PyIceberg\n",
    "\n",
    "Finally, we can get pyiceberg access to our data lake and utilize all our knowledge from the first part of the workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get direct pyiceberg access to the datalake\n",
    "iceberg_table = dataset_catalog.jaffle_shop_dataset.table_client.load_open_table(\"iceberg\", \"customers\")\n",
    "print(iceberg_table.location())\n",
    "print(iceberg_table.scan().to_arrow())\n",
    "\n",
    "# show catalog info\n",
    "iceberg_catalog = dataset_catalog.jaffle_shop_dataset.table_client.get_open_table_catalog(\"iceberg\")\n",
    "print(iceberg_catalog.list_namespaces())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÜ Final Exercise: Who‚Äôs the best customer?\n",
    "\n",
    "Using any of the data access methods (Python, Ibis, SQL, or PyIceberg), find the name of the customer who placed the most orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Running transformations on DuckDB views\n",
    "\n",
    "The next section is only relevant if you performed the Step 6 \"Run SQL Workflows with dbt\"!\n",
    "\n",
    "Let's explore the resulting reports dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt_portable_data_lake_demo\n",
    "dataset_catalog = dlt_portable_data_lake_demo.catalog(profile=\"dev\")\n",
    "print(dataset_catalog.reports_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_catalog.reports_dataset.dim_orders.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_catalog.reports_dataset.dim_customers.df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv kernel",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
